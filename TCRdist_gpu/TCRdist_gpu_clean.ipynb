{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66349d26-c609-4076-87c7-d46026f6377e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for available GPU...\n",
      "\n",
      "Apple Silicon GPU detected:\n",
      "Apple Silicon GPU (M1/M2/M3)\n",
      "Checking for GPU-related Python modules...\n",
      "\n",
      "'mlx' is installed (for Apple Silicon GPUs).\n",
      "Loading mlx to perform TCRdist\n"
     ]
    }
   ],
   "source": [
    "from TCRdist_gpu import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c66da2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_tcr = load_TCR_file()\n",
    "params_df, params_vec = load_params_file()\n",
    "submat = load_substitution_matrix()\n",
    "encoded1, tcr1 = encode_TCRs(tst_tcr, params_vec, n_max = 40000)\n",
    "encoded2, tcr2 = encode_TCRs(tst_tcr, params_vec, n_max = 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d60a402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of chunks (rows): 40\n",
      "total number of chunks (cols): 40\n",
      "Processing chunk (rows) 0\n",
      "Processing chunk (rows) 10000\n",
      "Processing chunk (rows) 20000\n",
      "Processing chunk (rows) 30000\n",
      "Time taken: 10.172045 seconds\n"
     ]
    }
   ],
   "source": [
    "scores = TCRdist_batch(tcr1, tcr2, submat = submat, tcrdist_cutoff=90, output = \"edge_list\", chunk_size=1000, print_chunk_size=10000, print_res = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7fe8fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edge1_0index</th>\n",
       "      <th>edge2_0index</th>\n",
       "      <th>TCRdist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [edge1_0index, edge2_0index, TCRdist]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[scores.TCRdist == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0087e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.to_parquet(\"../output/TCRdist_output/junk.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbaef15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_0index  col_0index  TCRdist\n",
      "0         143         144       81\n",
      "1         144         143       81\n",
      "2         259         267       69\n",
      "3         262         301       72\n",
      "4         267         259       69\n",
      "5         301         262       72\n",
      "6         572         971       64\n",
      "7         971         572       64\n",
      "<Compressed Sparse Row sparse matrix of dtype 'uint8'\n",
      "\twith 8 stored elements and shape (1000, 1000)>\n",
      "  Coords\tValues\n",
      "  (143, 144)\t81\n",
      "  (144, 143)\t81\n",
      "  (259, 267)\t69\n",
      "  (262, 301)\t72\n",
      "  (267, 259)\t69\n",
      "  (301, 262)\t72\n",
      "  (572, 971)\t64\n",
      "  (971, 572)\t64\n"
     ]
    }
   ],
   "source": [
    "tst_tcr = load_TCR_file()\n",
    "params_df, params_vec = load_params_file()\n",
    "submat = load_substitution_matrix()\n",
    "encoded1, tcr1 = encode_TCRs(tst_tcr, params_vec, n_max = 1000)\n",
    "encoded2, tcr2 = encode_TCRs(tst_tcr, params_vec, n_max = 1000)\n",
    "scores_sparse, scores_edge_list = TCRdist_simple(tcr1, tcr2, submat = submat, output = \"both\")\n",
    "print(scores_edge_list)\n",
    "print(scores_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc690c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of chunks (rows): 745\n",
      "total number of chunks (cols): 745\n",
      "Processing chunk (rows) 0\n",
      "Processing chunk (rows) 5000\n",
      "Processing chunk (rows) 10000\n",
      "Processing chunk (rows) 15000\n",
      "Processing chunk (rows) 20000\n",
      "Processing chunk (rows) 25000\n",
      "Processing chunk (rows) 30000\n",
      "Processing chunk (rows) 35000\n",
      "Processing chunk (rows) 40000\n",
      "Processing chunk (rows) 45000\n",
      "Processing chunk (rows) 50000\n",
      "Processing chunk (rows) 55000\n",
      "Processing chunk (rows) 60000\n",
      "Processing chunk (rows) 65000\n",
      "Processing chunk (rows) 70000\n",
      "Processing chunk (rows) 75000\n",
      "Processing chunk (rows) 80000\n",
      "Processing chunk (rows) 85000\n",
      "Processing chunk (rows) 90000\n",
      "Processing chunk (rows) 95000\n",
      "Processing chunk (rows) 100000\n",
      "Processing chunk (rows) 105000\n",
      "Processing chunk (rows) 110000\n",
      "Processing chunk (rows) 115000\n",
      "Processing chunk (rows) 120000\n",
      "Processing chunk (rows) 125000\n",
      "Processing chunk (rows) 130000\n",
      "Processing chunk (rows) 135000\n",
      "Processing chunk (rows) 140000\n",
      "Processing chunk (rows) 145000\n",
      "Processing chunk (rows) 150000\n",
      "Processing chunk (rows) 155000\n",
      "Processing chunk (rows) 160000\n",
      "Processing chunk (rows) 165000\n",
      "Processing chunk (rows) 170000\n",
      "Processing chunk (rows) 175000\n",
      "Processing chunk (rows) 180000\n",
      "Processing chunk (rows) 185000\n",
      "Processing chunk (rows) 190000\n",
      "Processing chunk (rows) 195000\n",
      "Processing chunk (rows) 200000\n",
      "Processing chunk (rows) 205000\n",
      "Processing chunk (rows) 210000\n",
      "Processing chunk (rows) 215000\n",
      "Processing chunk (rows) 220000\n",
      "Processing chunk (rows) 225000\n",
      "Processing chunk (rows) 230000\n",
      "Processing chunk (rows) 235000\n",
      "Processing chunk (rows) 240000\n",
      "Processing chunk (rows) 245000\n",
      "Processing chunk (rows) 250000\n",
      "Processing chunk (rows) 255000\n",
      "Processing chunk (rows) 260000\n",
      "Processing chunk (rows) 265000\n",
      "Processing chunk (rows) 270000\n",
      "Processing chunk (rows) 275000\n",
      "Processing chunk (rows) 280000\n",
      "Processing chunk (rows) 285000\n",
      "Processing chunk (rows) 290000\n",
      "Processing chunk (rows) 295000\n",
      "Processing chunk (rows) 300000\n",
      "Processing chunk (rows) 305000\n",
      "Processing chunk (rows) 310000\n",
      "Processing chunk (rows) 315000\n",
      "Processing chunk (rows) 320000\n",
      "Processing chunk (rows) 325000\n",
      "Processing chunk (rows) 330000\n",
      "Processing chunk (rows) 335000\n",
      "Processing chunk (rows) 340000\n",
      "Processing chunk (rows) 345000\n",
      "Processing chunk (rows) 350000\n",
      "Processing chunk (rows) 355000\n",
      "Processing chunk (rows) 360000\n",
      "Processing chunk (rows) 365000\n",
      "Processing chunk (rows) 370000\n",
      "Processing chunk (rows) 375000\n",
      "Processing chunk (rows) 380000\n",
      "Processing chunk (rows) 385000\n",
      "Processing chunk (rows) 390000\n",
      "Processing chunk (rows) 395000\n",
      "Processing chunk (rows) 400000\n",
      "Processing chunk (rows) 405000\n",
      "Processing chunk (rows) 410000\n",
      "Processing chunk (rows) 415000\n",
      "Processing chunk (rows) 420000\n",
      "Processing chunk (rows) 425000\n",
      "Processing chunk (rows) 430000\n",
      "Processing chunk (rows) 435000\n",
      "Processing chunk (rows) 440000\n",
      "Processing chunk (rows) 445000\n",
      "Processing chunk (rows) 450000\n",
      "Processing chunk (rows) 455000\n",
      "Processing chunk (rows) 460000\n",
      "Processing chunk (rows) 465000\n",
      "Processing chunk (rows) 470000\n",
      "Processing chunk (rows) 475000\n",
      "Processing chunk (rows) 480000\n",
      "Processing chunk (rows) 485000\n",
      "Processing chunk (rows) 490000\n",
      "Processing chunk (rows) 495000\n",
      "Processing chunk (rows) 500000\n",
      "Processing chunk (rows) 505000\n",
      "Processing chunk (rows) 510000\n",
      "Processing chunk (rows) 515000\n",
      "Processing chunk (rows) 520000\n",
      "Processing chunk (rows) 525000\n",
      "Processing chunk (rows) 530000\n",
      "Processing chunk (rows) 535000\n",
      "Processing chunk (rows) 540000\n",
      "Processing chunk (rows) 545000\n",
      "Processing chunk (rows) 550000\n",
      "Processing chunk (rows) 555000\n",
      "Processing chunk (rows) 560000\n",
      "Processing chunk (rows) 565000\n",
      "Processing chunk (rows) 570000\n",
      "Processing chunk (rows) 575000\n",
      "Processing chunk (rows) 580000\n",
      "Processing chunk (rows) 585000\n",
      "Processing chunk (rows) 590000\n",
      "Processing chunk (rows) 595000\n",
      "Processing chunk (rows) 600000\n",
      "Processing chunk (rows) 605000\n",
      "Processing chunk (rows) 610000\n",
      "Processing chunk (rows) 615000\n",
      "Processing chunk (rows) 620000\n",
      "Processing chunk (rows) 625000\n",
      "Processing chunk (rows) 630000\n",
      "Processing chunk (rows) 635000\n",
      "Processing chunk (rows) 640000\n",
      "Processing chunk (rows) 645000\n",
      "Processing chunk (rows) 650000\n",
      "Processing chunk (rows) 655000\n",
      "Processing chunk (rows) 660000\n",
      "Processing chunk (rows) 665000\n",
      "Processing chunk (rows) 670000\n",
      "Processing chunk (rows) 675000\n",
      "Processing chunk (rows) 680000\n",
      "Processing chunk (rows) 685000\n",
      "Processing chunk (rows) 690000\n",
      "Processing chunk (rows) 695000\n",
      "Processing chunk (rows) 700000\n",
      "Processing chunk (rows) 705000\n",
      "Processing chunk (rows) 710000\n",
      "Processing chunk (rows) 715000\n",
      "Processing chunk (rows) 720000\n",
      "Processing chunk (rows) 725000\n",
      "Processing chunk (rows) 730000\n",
      "Processing chunk (rows) 735000\n",
      "Processing chunk (rows) 740000\n",
      "Processing chunk (rows) 745000\n",
      "Time taken: 5386.128193 seconds\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'output/TCRdist_output'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m chunk_n = \u001b[32m1000\u001b[39m\n\u001b[32m      9\u001b[39m res = TCRdist_batch(tcr1, tcr2, submat = submat, tcrdist_cutoff=\u001b[32m90\u001b[39m, output = \u001b[33m\"\u001b[39m\u001b[33medge_list\u001b[39m\u001b[33m\"\u001b[39m, chunk_size=chunk_n, print_chunk_size=\u001b[32m5000\u001b[39m, print_res = \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput/TCRdist_output/covid_745k_tcrdist_cutoff_90.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/nick_main/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/nick_main/lib/python3.12/site-packages/pandas/core/frame.py:3113\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[39m\n\u001b[32m   3032\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3033\u001b[39m \u001b[33;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[32m   3034\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3109\u001b[39m \u001b[33;03m>>> content = f.read()\u001b[39;00m\n\u001b[32m   3110\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3111\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3113\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3114\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3121\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3122\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/nick_main/lib/python3.12/site-packages/pandas/io/parquet.py:480\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m impl = get_engine(engine)\n\u001b[32m    478\u001b[39m path_or_buf: FilePath | WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] = io.BytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io.BytesIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/nick_main/lib/python3.12/site-packages/pandas/io/parquet.py:198\u001b[39m, in \u001b[36mPyArrowImpl.write\u001b[39m\u001b[34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    195\u001b[39m     merged_metadata = {**existing_metadata, **df_metadata}\n\u001b[32m    196\u001b[39m     table = table.replace_schema_metadata(merged_metadata)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    206\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(path_or_handle, io.BufferedWriter)\n\u001b[32m    207\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(path_or_handle, \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    208\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_handle.name, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m))\n\u001b[32m    209\u001b[39m ):\n\u001b[32m    210\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_handle.name, \u001b[38;5;28mbytes\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/nick_main/lib/python3.12/site-packages/pandas/io/parquet.py:140\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    130\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    132\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    138\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    144\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/nick_main/lib/python3.12/site-packages/pandas/io/common.py:749\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression != \u001b[33m\"\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/nick_main/lib/python3.12/site-packages/pandas/io/common.py:616\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    614\u001b[39m parent = Path(path).parent\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: 'output/TCRdist_output'"
     ]
    }
   ],
   "source": [
    "### example with edge_list and batching\n",
    "from TCRdist_gpu import *\n",
    "tst_tcr = load_TCR_file()\n",
    "params_df, params_vec = load_params_file()\n",
    "submat = load_substitution_matrix()\n",
    "encoded1, tcr1 = encode_TCRs(tst_tcr, params_vec, n_max = np.inf)\n",
    "encoded2, tcr2 = encode_TCRs(tst_tcr, params_vec, n_max = np.inf)\n",
    "chunk_n = 1000\n",
    "res = TCRdist_batch(tcr1, tcr2, submat = submat, tcrdist_cutoff=90, output = \"edge_list\", chunk_size=chunk_n, print_chunk_size=5000, print_res = True)\n",
    "res.to_parquet(\"../output/TCRdist_output/covid_745k_tcrdist_cutoff_90.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27bcf84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_parquet(\"../output/TCRdist_output/covid_745k_tcrdist_cutoff_90.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cceb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### note: parquet is smaller than sparse csr matrix, so just write parquet\n",
    "#coo_mat = scipy.sparse.coo_matrix((res['TCRdist'], (res['row_0index'], res['col_0index'])))\n",
    "#csr_mat = coo_mat.tocsr()\n",
    "#scipy.io.mmwrite(\"covid_745k_tcrdist_cutoff_120.mtx\", csr_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f13475d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### example with covid and vdj inputs\n",
    "from TCRdist_gpu import *\n",
    "tcr_both = pd.read_parquet(os.path.join(\"data\", \"vdj_and_covid_032425.parquet\"))\n",
    "params_df, params_vec = load_params_file()\n",
    "submat = load_substitution_matrix()\n",
    "encoded1, tcr1 = encode_TCRs(tcr_both, params_vec, n_max = np.inf)\n",
    "encoded2, tcr2 = encode_TCRs(tcr_both, params_vec, n_max = np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12fe6680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for available GPU...\n",
      "\n",
      "Apple Silicon GPU detected:\n",
      "Apple Silicon GPU (M1/M2/M3)\n",
      "Checking for GPU-related Python modules...\n",
      "\n",
      "'mlx' is installed (for Apple Silicon GPUs).\n",
      "Loading mlx to perform TCRdist\n",
      "total number of chunks (rows): 19\n",
      "total number of chunks (cols): 19\n",
      "Processing chunk (rows) 0\n",
      "Processing chunk (rows) 1000\n",
      "Processing chunk (rows) 2000\n",
      "Processing chunk (rows) 3000\n",
      "Processing chunk (rows) 4000\n",
      "Processing chunk (rows) 5000\n",
      "Processing chunk (rows) 6000\n",
      "Processing chunk (rows) 7000\n",
      "Processing chunk (rows) 8000\n",
      "Processing chunk (rows) 9000\n",
      "Processing chunk (rows) 10000\n",
      "Processing chunk (rows) 11000\n",
      "Processing chunk (rows) 12000\n",
      "Processing chunk (rows) 13000\n",
      "Processing chunk (rows) 14000\n",
      "Processing chunk (rows) 15000\n",
      "Processing chunk (rows) 16000\n",
      "Processing chunk (rows) 17000\n",
      "Processing chunk (rows) 18000\n",
      "Processing chunk (rows) 19000\n",
      "Time taken: 2.668478 seconds\n"
     ]
    }
   ],
   "source": [
    "from TCRdist_gpu import *\n",
    "tcr_both = pd.read_parquet(os.path.join(\"data\", \"vdj_and_covid_032425.parquet\"))\n",
    "tcr1 = tcr_both[1:20000]\n",
    "tcr2 = tcr_both[1:20000]\n",
    "params_df, _ = load_params_file()\n",
    "#submat = np.loadtxt(os.path.join(\"data\", 'TCRdist_matrix_mega.tsv'), delimiter='\\t', dtype=np.int16)\n",
    "submat = load_substitution_matrix()\n",
    "chunk_n = 1000\n",
    "res = TCRdist_batch(tcr1, tcr2, submat = submat, params_df = params_df, tcrdist_cutoff=90, output = \"edge_list\", chunk_size=chunk_n, print_chunk_size=chunk_n, print_res = True)\n",
    "#chunk_n = 1000\n",
    "#res = TCRdist_batch(tcr1, tcr2, submat = submat, tcrdist_cutoff=90, output = \"edge_list\", chunk_size=chunk_n, print_chunk_size=chunk_n, print_res = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d4650ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(submat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba9a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_parquet(\"../output/TCRdist_output/covid_to_vdjdb_tcrdist_cutoff_90.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nick_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
